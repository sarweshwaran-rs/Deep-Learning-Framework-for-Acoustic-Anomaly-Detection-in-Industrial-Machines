{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8286277",
   "metadata": {},
   "source": [
    "# Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80c38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, balanced_accuracy_score, f1_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad110b3",
   "metadata": {},
   "source": [
    "# Custom Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4fa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_modules_path = os.path.abspath(r'F:\\Capstone\\DFCA')\n",
    "\n",
    "# Add the path to sys.path\n",
    "if custom_modules_path not in sys.path:\n",
    "    sys.path.append(custom_modules_path)\n",
    "\n",
    "from utils.datasets import PairedSpectrogramDataset, WindowedPairedSpectrogramDataset\n",
    "from utils.metrics_utils import calculate_pAUC, plot_confusion_matrix\n",
    "from scripts.pretrain_pipeline import FusedModel\n",
    "from models.heads import AnomalyScorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeb35cf",
   "metadata": {},
   "source": [
    "# Configuration's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89ced13",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\" + (f\" - {torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"\"))\n",
    "WINDOW_SIZE = 5\n",
    "BATCH_SIZE=32\n",
    "USE_TEMPORAL_DECODER = True \n",
    "SEQ_LOSS_WEIGHT = 0.3\n",
    "save_path = os.path.abspath(r\"F:\\CapStone\\DFCA\\checkpoints\\Classifier[0_dB_valve]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9721f491",
   "metadata": {},
   "source": [
    "# Helper Function's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b100a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_primary_probs_and_loss_from_head(outputs, labels, criterion):\n",
    "    \"\"\"\n",
    "        Returns: probs [B], preds [B], loss (scalar)\n",
    "        Assumes outputs is:\n",
    "            - logits tensor for classifier/mlp/classifier-1\n",
    "            - distance/anomaly score for prototype (AnomalyScorer prototype)\n",
    "            - embeddings for embedding head (handled separately!)\n",
    "    \"\"\"\n",
    "    # ===== DEBUG PRINT ===================\n",
    "    # print(\"Logits shape: \", outputs.shape)\n",
    "    # print(\"Labels shape: \",labels.shape)\n",
    "    # ===== DEBUG PRINT ===================\n",
    "    logits = outputs.squeeze()\n",
    "    probs = torch.sigmoid(logits)\n",
    "    loss = criterion(logits, labels.float())\n",
    "    preds = (probs > 0.5).long()\n",
    "    \n",
    "    return probs, preds, loss\n",
    "\n",
    "def _temporal_aux_loss(seq_scores, labels, criterions_for_seq):\n",
    "    \"\"\"\n",
    "        seq_scores: (B, T) raw logits from TemporalSmoothingDEcoder (Linear output)\n",
    "        labels: (B, ) => expand to (B, T)\n",
    "        criterion_for_seq: BCEWithLogitsLoss (or similar) for temporal smoothing\n",
    "    \"\"\"\n",
    "    if seq_scores.ndim == 2:\n",
    "        B, T = seq_scores.shape\n",
    "        labels_T = labels.float().unsqueeze(1).expand(B, T)\n",
    "    \n",
    "    elif seq_scores.ndim == 1:\n",
    "        B = seq_scores.shape[0]\n",
    "        T = 1\n",
    "        seq_scores = seq_scores.unsqueeze(1)\n",
    "        labels_T = labels.float().unsqueeze(1)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected seq_scores shape {seq_scores.shape}\")\n",
    "    \n",
    "    aux_loss = criterions_for_seq(seq_scores, labels_T)\n",
    "    # derive a sequence-level probability for metrics by averaging sigmoid(seq_scores)\n",
    "    seq_probs = torch.sigmoid(seq_scores).mean(dim=1)\n",
    "    \n",
    "    return aux_loss, seq_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341d0dca",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29417289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion, phase=\"Evaluation\", device=\"cpu\", sample_count=10, threshold=0.5, use_temporal=False, aux_seq_weight=SEQ_LOSS_WEIGHT):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_labels, all_probs = [], []\n",
    "    best_threshold = threshold\n",
    "    f1 = 0.0\n",
    "\n",
    "    # For temporal aux loss\n",
    "    seq_criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    class_counts = {0: 0, 1:0}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch  in tqdm(data_loader, desc=phase):\n",
    "            stft = batch['stft'].to(device)\n",
    "            cqt = batch['cqt'].to(device)\n",
    "            labels = batch['label'].to(device).long()\n",
    "\n",
    "            for lbl in labels.cpu().numpy().flatten():\n",
    "                class_counts[int(lbl.item())] += 1\n",
    "            \n",
    "            if use_temporal:\n",
    "                head_out, seq_scores = model(stft,cqt) # head_out: [B, ?], seq_scores: (B, T)\n",
    "                probs_primary, preds_primary, primary_loss = _compute_primary_probs_and_loss_from_head(\n",
    "                    head_out, labels, criterion\n",
    "                )\n",
    "                # auxiliary temporal smoothing\n",
    "                aux_loss, seq_probs = _temporal_aux_loss(seq_scores, labels, seq_criterion)\n",
    "\n",
    "                # Merge probs for metric (bend primary with sequence; kep primary dominant)\n",
    "                probs = 0.7 * probs_primary + 0.3 * seq_probs\n",
    "                loss = primary_loss + aux_seq_weight * aux_loss\n",
    "            \n",
    "            \n",
    "            running_loss += loss.item() * stft.size(0)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.detach().cpu().numpy())\n",
    "\n",
    "    print(f\"[DEBUG] {phase} label counts: {class_counts}\")\n",
    "\n",
    "    # Optimal threshold sweep on Validation\n",
    "    f1 = 0.0\n",
    "    if phase == \"Validation\":\n",
    "        best_f1 = 0\n",
    "        current_optimal_threshold = 0.5\n",
    "        for thresh in np.arange(0.01, 1.0, 0.01):\n",
    "            predictions_thresh = (np.array(all_probs) > thresh).astype(int)\n",
    "            f1_candidate = f1_score(all_labels, predictions_thresh)\n",
    "\n",
    "            if f1_candidate > best_f1:\n",
    "                best_f1 = f1_candidate\n",
    "                current_optimal_threshold = thresh\n",
    "        best_threshold = current_optimal_threshold\n",
    "        f1 = best_f1\n",
    "        print(f\"Optimal Threshold (F1-score): {best_threshold:.2f}\")\n",
    "        print(f\"Best F1-score on Validation Set: {best_f1:.4f}\")\n",
    "\n",
    "    # Metrics under chosen threshold\n",
    "    all_preds = (np.array(all_probs) > best_threshold).astype(int)\n",
    "    if phase != \"Validation\":\n",
    "        if len(np.unique(all_labels)) > 1:\n",
    "            f1 = f1_score(all_labels, all_preds)\n",
    "        else:\n",
    "            f1 = 0.0\n",
    "\n",
    "    avg_loss = running_loss / len(data_loader.dataset)\n",
    "    auc_score = roc_auc_score(all_labels, all_probs) if len(np.unique(all_labels)) > 1 else float('nan')\n",
    "    acc_score = accuracy_score(all_labels, all_preds)\n",
    "    bacc_score = balanced_accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"{phase} Loss: {avg_loss:.4f} | {phase} AUC: {auc_score:.4f} | {phase} ACC: {acc_score:.4f} | {phase} BACC: {bacc_score:.4f}\")\n",
    "    print(f\"[DEBUG] {phase} Prediction Distribution: {dict(Counter(all_preds))}\")\n",
    "    print(f\"[DEBUG] {phase} Label Distribution: {dict(Counter(all_labels))}\")\n",
    "    print(\"==================== Misclassification & Samples ====================\")\n",
    "    errors = [(i, p, pr, l) for i, (p, pr, l) in enumerate(zip(all_preds, all_probs, all_labels)) if p != l]\n",
    "    print(f\"{phase} Misclassified Samples: {len(errors)} / {len(all_labels)}\")\n",
    "    print(\"\\nSample Predictions Vs Lables:\")\n",
    "    for i in range(min(10, len(all_labels))):\n",
    "        print(f\"Sample {i+1}: Pred = {all_preds[i]}, Prob = {all_probs[i]:.4f}, True = {all_labels[i]}\")\n",
    "    \n",
    "    return avg_loss, auc_score, acc_score, bacc_score, f1, all_labels, all_probs, best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab809fe7",
   "metadata": {},
   "source": [
    "# Function to test on new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0de140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset(model, features_dir, model_path, best_threshold, device):\n",
    "    \"\"\"\n",
    "        Loads a trained model and evaluate it on a new dataset directory.\n",
    "        Args:\n",
    "            model (nn.Module): The model architectue.\n",
    "            features_dir(str): Path to the directory containing the new test features.\n",
    "            model_path(str): Path to saved .pth model file.\n",
    "            best_threshold(float): The optimal threshold found during training\n",
    "            device(torch.device): The device to run evaluation on ('cuda' or 'cpu')\n",
    "    \"\"\"\n",
    "    print(f\"=== Evaluating on:{os.path.basename(features_dir)} ===\")\n",
    "\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n",
    "        model.to(device)\n",
    "        print(f\"Model Loaded successfully from {model_path}\")\n",
    "    except Exception as error:\n",
    "        print(f\"Error in loading model from {model_path}: {error}\")\n",
    "        return\n",
    "    \n",
    "    new_base_dataset = PairedSpectrogramDataset(base_dir=features_dir, transform=None)\n",
    "    new_test_set = WindowedPairedSpectrogramDataset(base_dataset=new_base_dataset, window_size=WINDOW_SIZE)\n",
    "    new_test_loader = DataLoader(dataset=new_test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    print(f\"Test Set Size: {len(new_test_set)}\")\n",
    "    print(f\"Label Distribution: {Counter(new_base_dataset.labels)}\")\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.0], dtype=torch.float32).to(device))\n",
    "\n",
    "    test_loss, test_auc, test_acc, test_bacc, test_f1, all_labels, all_probs, _ = evaluate_model(\n",
    "        model=model, data_loader=new_test_loader, criterion=criterion, phase=\"Test\",\n",
    "        device=device, threshold=best_threshold, use_temporal=USE_TEMPORAL_DECODER, aux_seq_weight=SEQ_LOSS_WEIGHT\n",
    "    )\n",
    "\n",
    "    if len(np.unique(all_labels)) > 1:\n",
    "        pauc_score = calculate_pAUC(labels=all_labels, preds=all_probs, max_fpr=0.2)\n",
    "        print(f\"Partial AUC (pAUC @ 0.2 FPR): {pauc_score:.4f}\")\n",
    "\n",
    "    else:\n",
    "        pauc_score=float('nan')\n",
    "        print(\"Test set contains only one class; Cannot compute pAUC\")\n",
    "\n",
    "    print(f\"\\n Final Test Metrics (Threshold: {best_threshold:.2f})\")\n",
    "    print(f\"Loss:{test_loss:.4f} | AUC:{test_auc:.4f} | Accuracy: {test_acc:.4f} | Balanced Accuracy:{test_bacc:.4f} | F1-Score:{test_f1:.4f} | pAUC:{pauc_score:.4f}\")\n",
    "    \n",
    "\n",
    "    labels_display = [\"Normal\", \"Abnormal\"]\n",
    "    all_preds = (np.array(all_probs) > best_threshold).astype(int)\n",
    "    plot_confusion_matrix(y_true=all_labels, y_pred=all_preds, labels=labels_display, save_path=save_image_path, title=f\"Confusion Matrix - {os.path.basename(features_dir)}\") # type: ignore\n",
    "    print(f\"\\nCompleted Test on {features_dir}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7e67e4",
   "metadata": {},
   "source": [
    "# Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab8d3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\" + (f\" - {torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"\"))\n",
    "    save_image_path = os.path.abspath(r\"F:\\Capstone\\DFCA\\checkpoints\\Results\\[6 and -6_valve](0.07)\")\n",
    "    save_image_path = os.path.join(save_image_path,'AUC_Model')\n",
    "    # save_image_path = os.path.join(save_image_path,'BACC_Model')\n",
    "    os.makedirs(save_image_path, exist_ok=True)\n",
    "    print(f\"Image Saving Path: {save_image_path}\")\n",
    "    SAVED_MODEL_PATH = os.path.join(save_path, \"best_model.pth\")\n",
    "    # SAVED_MODEL_PATH = os.path.join(save_path, \"best_bacc.pth\")\n",
    "    OPTIMAL_THRESHOLD = 0.07\n",
    "    # Base directory\n",
    "    BASE_FEATURES_DIRECTORY = os.path.abspath(r'F:\\Capstone\\DFCA\\data\\features')\n",
    "\n",
    "    # Paths to the new test directories\n",
    "    SIX_DB_VALVE_DIR = os.path.join(BASE_FEATURES_DIRECTORY, '6_dB_valve_features')\n",
    "    MINUS_SIX_DB_VALVE_DIR = os.path.join(BASE_FEATURES_DIRECTORY,'-6_dB_valve_features')\n",
    "\n",
    "    # Instiate of head\n",
    "    head = AnomalyScorer(in_dim=256, dropout=0.4, mode='classifier-1')\n",
    "    model = FusedModel(stft_dim=512, cqt_dim=320, fusion_dim=256, head=head, use_decoder=USE_TEMPORAL_DECODER, temporal_hidden=64)\n",
    "    \n",
    "    # ====== Test On 6_dB_valve_features dataset =====\n",
    "    if os.path.isdir(SIX_DB_VALVE_DIR):\n",
    "        test_dataset(\n",
    "            model=model,\n",
    "            features_dir=SIX_DB_VALVE_DIR,\n",
    "            model_path=SAVED_MODEL_PATH,\n",
    "            best_threshold=OPTIMAL_THRESHOLD,\n",
    "            device=device\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Directory not found: {SIX_DB_VALVE_DIR}\")\n",
    "\n",
    "    # ====== Test On -6_dB_valve_features dataset =====\n",
    "    if os.path.isdir(MINUS_SIX_DB_VALVE_DIR):\n",
    "        test_dataset(\n",
    "            model=model,\n",
    "            features_dir=MINUS_SIX_DB_VALVE_DIR,\n",
    "            model_path=SAVED_MODEL_PATH,\n",
    "            best_threshold=OPTIMAL_THRESHOLD,\n",
    "            device=device\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Directory not found: {MINUS_SIX_DB_VALVE_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
