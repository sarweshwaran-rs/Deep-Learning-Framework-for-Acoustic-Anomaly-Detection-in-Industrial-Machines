{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fa736d2",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "# Importing the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df53743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, balanced_accuracy_score, roc_curve, f1_score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c803927f",
   "metadata": {},
   "source": [
    "# Importing the Custom Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e3a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_modules_path = os.path.abspath(r'F:\\Capstone\\DFCA')\n",
    "\n",
    "# Add the path to sys.path\n",
    "if custom_modules_path not in sys.path:\n",
    "    sys.path.append(custom_modules_path)\n",
    "\n",
    "from utils.datasets import PairedSpectrogramDataset, WindowedPairedSpectrogramDataset\n",
    "from utils.augmentations import ComposeT, ToTensor, SpecAugment, SpecTimePitchWarp\n",
    "from utils.gradcam_utils import build_gradcam_for_model, run_and_save_gradcams\n",
    "from utils.metrics_utils import calculate_pAUC, plot_confusion_matrix\n",
    "from scripts.pretrain_pipeline import FusedModel \n",
    "from models.heads import SimpleAnomalyMLP, ComplexAnomalyMLP, EmbeddingMLP, AnomalyScorer\n",
    "from models.losses import ContrastiveLoss, BinaryFocalLoss, FocalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d683c6b",
   "metadata": {},
   "source": [
    "# Configuration's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931cd01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\" + (f\" - {torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"\"))\n",
    "\n",
    "FEATURES_DIR = os.path.abspath(r'F:\\CapStone\\DFCA\\data\\features\\-6_dB_features')\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHs = 5\n",
    "LR = 5e-5\n",
    "WEIGHT_DECAY = 1e-2\n",
    "CHECKPOINT_DIR = os.path.abspath(r'F:\\Capstone\\DFCA\\checkpoints')\n",
    "CONTRASTIVE_MARGIN = 0.5\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "HEAD_MODE = 'mlp' # 'mlp', 'classifier', 'classifier-1', 'prototype','embedding'\n",
    "EMB_DIM = 64\n",
    "\n",
    "USE_TEMPORAL_DECODER = True\n",
    "WINDOW_SIZE = 5\n",
    "SEQ_LOSS_WEIGHT = 0.3\n",
    "\n",
    "save_path = os.path.join(CHECKPOINT_DIR, '[Anomaly-With-Transformations-dropout=0.4]_MLP(5e-5)')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "if not USE_TEMPORAL_DECODER:\n",
    "    print(f\"Learning Rate: {LR} | Weight decay: {WEIGHT_DECAY} | SEQ Loss Weight: {SEQ_LOSS_WEIGHT} | Head Mode: {HEAD_MODE}\")\n",
    "else:\n",
    "    print(f\"Learning Rate: {LR} | Weight decay: {WEIGHT_DECAY} | SEQ Loss Weight: {SEQ_LOSS_WEIGHT} | Head Mode: {HEAD_MODE} |Window size: {WINDOW_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22ae5dc",
   "metadata": {},
   "source": [
    "# Helper Function's Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179c0206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# Evaluation (Supports both modes)\n",
    "# ==================================\n",
    "def _compute_primary_probs_and_loss_from_head(head_mode, outputs, labels, criterion):\n",
    "    \"\"\"\n",
    "        Returns: probs [B], preds [B], loss (scalar)\n",
    "        Assumes outputs is:\n",
    "            - logits tensor for classifier/mlp/classifier-1\n",
    "            - distance/anomaly score for prototype (AnomalyScorer prototype)\n",
    "            - embeddings for embedding head (handled separately!)\n",
    "    \"\"\"\n",
    "    # ===== DEBUG PRINT ===================\n",
    "    # print(\"Logits shape: \", outputs.shape)\n",
    "    # print(\"Labels shape: \",labels.shape)\n",
    "    # ===== DEBUG PRINT ===================\n",
    "    if head_mode == \"classifier\":\n",
    "        if outputs.ndim == 2 and outputs.shape[1] == 2:\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            preds = torch.argmax(outputs.detach(), dim=1)\n",
    "        else:\n",
    "            logits = outputs.squeeze()\n",
    "            probs = torch.sigmoid(logits)\n",
    "            loss = criterion(logits, labels.float())\n",
    "            preds = (probs > 0.5).long()\n",
    "    \n",
    "    elif head_mode == \"classifier-1\":\n",
    "        logits = outputs.squeeze()\n",
    "        probs = torch.sigmoid(logits)\n",
    "        loss = criterion(logits, labels.float())\n",
    "        preds = (probs > 0.5).long()\n",
    "\n",
    "    elif head_mode == \"mlp\":\n",
    "        logits = outputs.squeeze(1) if outputs.ndim == 2 else outputs\n",
    "        probs = torch.sigmoid(logits)\n",
    "        loss = criterion(logits, labels.float())\n",
    "        preds = (probs > 0.5).long()\n",
    "\n",
    "    elif head_mode == \"prototype\":\n",
    "        # AnomalyScorer(mode = 'prototype') returns distances (higher => more anomalous)\n",
    "        scores = outputs.view(-1)\n",
    "        probs = torch.sigmoid(scores) # map to [0,1]; threshold later\n",
    "        loss = criterion(scores, labels.float()) # BCE/BinaryFocalLoss\n",
    "        preds = (probs > 0.5).long()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported head_mode in primary loss: {head_mode}\")\n",
    "    \n",
    "    return probs, preds, loss\n",
    "\n",
    "def _temporal_aux_loss(seq_scores, labels, criterions_for_seq):\n",
    "    \"\"\"\n",
    "        seq_scores: (B, T) raw logits from TemporalSmoothingDEcoder (Linear output)\n",
    "        labels: (B, ) => expand to (B, T)\n",
    "        criterion_for_seq: BCEWithLogitsLoss (or similar) for temporal smoothing\n",
    "    \"\"\"\n",
    "    if seq_scores.ndim == 2:\n",
    "        B, T = seq_scores.shape\n",
    "        labels_T = labels.float().unsqueeze(1).expand(B, T)\n",
    "    \n",
    "    elif seq_scores.ndim == 1:\n",
    "        B = seq_scores.shape[0]\n",
    "        T = 1\n",
    "        seq_scores = seq_scores.unsqueeze(1)\n",
    "        labels_T = labels.float().unsqueeze(1)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected seq_scores shape {seq_scores.shape}\")\n",
    "    \n",
    "    aux_loss = criterions_for_seq(seq_scores, labels_T)\n",
    "    # derive a sequence-level probability for metrics by averaging sigmoid(seq_scores)\n",
    "    seq_probs = torch.sigmoid(seq_scores).mean(dim=1)\n",
    "    \n",
    "    return aux_loss, seq_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d445afa6",
   "metadata": {},
   "source": [
    "# Evaluate and Train Function's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a795063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion, phase=\"Evaluation\", device=device, head_mode = 'classifier', sample_count=10, threshold=0.5, use_temporal=False, aux_seq_weight=SEQ_LOSS_WEIGHT):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_labels, all_probs = [], []\n",
    "    best_threshold = threshold\n",
    "    f1 = 0.0\n",
    "\n",
    "    # For temporal aux loss\n",
    "    seq_criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    class_counts = {0: 0, 1:0}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch  in tqdm(data_loader, desc=phase):\n",
    "            stft = batch['stft'].to(device)\n",
    "            cqt = batch['cqt'].to(device)\n",
    "            labels = batch['label'].to(device).long()\n",
    "\n",
    "            for lbl in labels.cpu().numpy().flatten():\n",
    "                class_counts[int(lbl.item())] += 1\n",
    "            \n",
    "            if use_temporal:\n",
    "                head_out, seq_scores = model(stft,cqt) # head_out: [B, ?], seq_scores: (B, T)\n",
    "                # primary (from head_out)\n",
    "                if head_mode == \"embedding\":\n",
    "                    # Embedding path uses ContrastiveLoss\n",
    "                    embeddings = head_out\n",
    "                    normal_proto = model.head.normal_prototype\n",
    "                    embeddings = F.normalize(embeddings, dim=1)\n",
    "                    normal_proto = F.normalize(normal_proto, dim=0)\n",
    "                    cos_sim = torch.sum(embeddings * normal_proto.unsqueeze(0).expand_as(embeddings), dim=1)\n",
    "                    probs_primary = 1 - cos_sim\n",
    "                    primary_loss = criterion(embeddings, normal_proto, labels) if isinstance(criterion, ContrastiveLoss) \\\n",
    "                                    else criterion(probs_primary, labels.float())\n",
    "                \n",
    "                else:\n",
    "                    probs_primary, preds_primary, primary_loss = _compute_primary_probs_and_loss_from_head(\n",
    "                        head_mode, head_out, labels, criterion\n",
    "                    )\n",
    "                    # auxiliary temporal smoothing\n",
    "                    aux_loss, seq_probs = _temporal_aux_loss(seq_scores, labels, seq_criterion)\n",
    "\n",
    "                    # Merge probs for metric (bend primary with sequence; kep primary dominant)\n",
    "                    probs = 0.7 * probs_primary + 0.3 * seq_probs\n",
    "                    loss = primary_loss + aux_seq_weight * aux_loss\n",
    "            \n",
    "            else:\n",
    "                outputs = model(stft, cqt)\n",
    "\n",
    "                if head_mode == \"embedding\":\n",
    "                    embeddings = outputs\n",
    "                    normal_proto = model.head.normal_prototype\n",
    "                    embeddings = F.normalize(embeddings, dim=1)\n",
    "                    normal_proto = F.normalize(normal_proto, dim=0)\n",
    "                    cos_sim = torch.sum(embeddings * normal_proto.unsqueeze(0).expand_as(embeddings), dim=1)\n",
    "                    probs = 1 - cos_sim\n",
    "                    loss = criterion(embeddings, normal_proto, labels) if isinstance(criterion, ContrastiveLoss) \\\n",
    "                           else criterion(probs, labels.float())\n",
    "\n",
    "                else:\n",
    "                    probs, preds_tmp, loss = _compute_primary_probs_and_loss_from_head(\n",
    "                        head_mode, outputs, labels, criterion\n",
    "                    )\n",
    "\n",
    "            running_loss += loss.item() * stft.size(0)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.detach().cpu().numpy())\n",
    "\n",
    "    print(f\"[DEBUG] {phase} label counts: {class_counts}\")\n",
    "\n",
    "    # Optimal threshold sweep on Validation\n",
    "    f1 = 0.0\n",
    "    if phase == \"Validation\":\n",
    "        best_f1 = 0\n",
    "        current_optimal_threshold = 0.5\n",
    "        for thresh in np.arange(0.01, 1.0, 0.01):\n",
    "            predictions_thresh = (np.array(all_probs) > thresh).astype(int)\n",
    "            f1_candidate = f1_score(all_labels, predictions_thresh)\n",
    "\n",
    "            if f1_candidate > best_f1:\n",
    "                best_f1 = f1_candidate\n",
    "                current_optimal_threshold = thresh\n",
    "        best_threshold = current_optimal_threshold\n",
    "        f1 = best_f1\n",
    "        print(f\"Optimal Threshold (F1-score): {best_threshold:.2f}\")\n",
    "        print(f\"Best F1-score on Validation Set: {best_f1:.4f}\")\n",
    "\n",
    "    # Metrics under chosen threshold\n",
    "    all_preds = (np.array(all_probs) > best_threshold).astype(int)\n",
    "    if phase != \"Validation\":\n",
    "        if len(np.unique(all_labels)) > 1:\n",
    "            f1 = f1_score(all_labels, all_preds)\n",
    "        else:\n",
    "            f1 = 0.0\n",
    "\n",
    "    avg_loss = running_loss / len(data_loader.dataset)\n",
    "    auc_score = roc_auc_score(all_labels, all_probs) if len(np.unique(all_labels)) > 1 else float('nan')\n",
    "    acc_score = accuracy_score(all_labels, all_preds)\n",
    "    bacc_score = balanced_accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"{phase} Loss: {avg_loss:.4f} | {phase} AUC: {auc_score:.4f} | {phase} ACC: {acc_score:.4f} | {phase} BACC: {bacc_score:.4f}\")\n",
    "    print(f\"[DEBUG] {phase} Prediction Distribution: {dict(Counter(all_preds))}\")\n",
    "    print(f\"[DEBUG] {phase} Label Distribution: {dict(Counter(all_labels))}\")\n",
    "    print(\"==================== Misclassification & Samples ====================\")\n",
    "    errors = [(i, p, pr, l) for i, (p, pr, l) in enumerate(zip(all_preds, all_probs, all_labels)) if p != l]\n",
    "    print(f\"{phase} Misclassified Samples: {len(errors)} / {len(all_labels)}\")\n",
    "    print(\"\\nSample Predictions Vs Lables:\")\n",
    "    for i in range(min(10, len(all_labels))):\n",
    "        print(f\"Sample {i+1}: Pred = {all_preds[i]}, Prob = {all_probs[i]:.4f}, True = {all_labels[i]}\")\n",
    "    print(\"=====================================================================\")\n",
    "\n",
    "    return avg_loss, auc_score, acc_score, bacc_score, f1, all_labels, all_probs, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfbe15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, head_mode, schedular=None, num_epochs=5, model_save_path=\"best_model.pth\", device=device, save_plots=True, use_temporal=False, aux_seq_weight=SEQ_LOSS_WEIGHT):\n",
    "    best_val_auc = -np.inf\n",
    "    best_val_loss = np.inf\n",
    "    current_threshold = 0.5\n",
    "    best_threshold = 0.5\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_aucs, val_aucs = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    train_baccs, val_baccs = [], []\n",
    "\n",
    "    model.to(device)\n",
    "    seq_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_labels, all_probs, all_preds = [], [], []\n",
    "\n",
    "        class_couns_train = {0:0, 1:0}\n",
    "        epoch_stats = defaultdict(list)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch in tqdm(train_loader, desc=\"Train\"):\n",
    "            stft = batch['stft'].to(device)\n",
    "            cqt = batch['cqt'].to(device)\n",
    "            labels = batch['label'].to(device).long()\n",
    "\n",
    "            for lbl in labels.cpu().numpy().flatten():\n",
    "                class_couns_train[int(lbl.item())] += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if use_temporal:\n",
    "                head_out, seq_scores = model(stft, cqt) # (B, ?), (B, T)\n",
    "\n",
    "                if head_mode == \"embedding\":\n",
    "                    embeddings = head_out\n",
    "                    normal_proto = model.head.normal_prototype\n",
    "                    embeddings = F.normalize(embeddings, dim=1)\n",
    "                    normal_proto = F.normalize(normal_proto, dim=0)\n",
    "                    cos_sim = torch.sum(embeddings * normal_proto.unsqueeze(0).expand_as(embeddings), dim=1)\n",
    "                    normal_sim = cos_sim[labels == 0].mean().item() if(labels == 0).any() else None\n",
    "                    anomaly_sim = cos_sim[labels == 1].mean().item() if (labels == 1).any() else None\n",
    "                    if normal_sim is not None: epoch_stats['normal_sim'].append(normal_sim)\n",
    "                    if anomaly_sim is not None: epoch_stats['anomaly_sim'].append(anomaly_sim)\n",
    "\n",
    "                    probs_primary = 1 - cos_sim\n",
    "                    primary_loss = criterion(embeddings, normal_proto, labels) if isinstance(criterion, ContrastiveLoss) \\\n",
    "                                   else criterion(probs_primary, labels.float())\n",
    "                    preds = (probs_primary > current_threshold).long()\n",
    "                    probs = probs_primary\n",
    "                \n",
    "                else:\n",
    "                    probs, preds, primary_loss = _compute_primary_probs_and_loss_from_head(\n",
    "                        head_mode, head_out, labels, criterion\n",
    "                    )\n",
    "\n",
    "                    aux_loss, seq_probs = _temporal_aux_loss(seq_scores, labels, seq_criterion)\n",
    "                    loss = primary_loss + aux_seq_weight * aux_loss\n",
    "\n",
    "                    # Fuse probs for metrics\n",
    "                    probs = 0.7 * probs + 0.3 * seq_probs\n",
    "            else:\n",
    "                outputs = model(stft, cqt)\n",
    "                if head_mode == \"classifier\":\n",
    "                    if outputs.ndim == 2 and outputs.shape[1] == 2:\n",
    "                        probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "                        preds = torch.argmax(outputs.detach(), dim=1)\n",
    "                        labels = criterion(outputs, labels)\n",
    "                    else:\n",
    "                        probs = torch.sigmoid(outputs.squeeze())\n",
    "                        preds = (probs > current_threshold).long()\n",
    "                        loss = criterion(outputs.squeeze(), labels.float())\n",
    "                \n",
    "                elif head_mode == \"classifier-1\":\n",
    "                    logits = outputs.squeeze()\n",
    "                    probs = torch.sigmoid(logits)\n",
    "                    preds = (probs > current_threshold).long()\n",
    "                    loss = criterion(logits, labels.float())\n",
    "                \n",
    "                elif head_mode == \"mlp\":\n",
    "                    logits = outputs.squeeze(1) if outputs.ndim == 2 else outputs\n",
    "                    probs = torch.sigmoid(logits)\n",
    "                    preds = (probs > current_threshold).long()\n",
    "                    loss = criterion(logits, labels.float())\n",
    "                \n",
    "                elif head_mode == \"prototype\":\n",
    "                    # Distance-based anomaly score from AnomalyScorer prototype\n",
    "                    scores = outputs.view(-1)\n",
    "                    probs = torch.sigmoid(scores)\n",
    "                    preds = (probs > current_threshold).long()\n",
    "                    loss = criterion(scores, labels.float()) # BCE/BinaryFocalLoss\n",
    "                \n",
    "                elif head_mode == \"embedding\":\n",
    "                    embeddings = outputs\n",
    "                    normal_proto = model.head.normal_prototype\n",
    "                    embeddings = F.normalize(embeddings, dim=1)\n",
    "                    normal_proto = F.normalize(normal_proto, dim=0)\n",
    "                    cos_sim = torch.sum(embeddings * normal_proto.unsqueeze(0).expand_as(embeddings), dim=1)\n",
    "                    normal_sim = cos_sim[labels == 0].mean().item() if (labels == 0).any() else None\n",
    "                    anomaly_sim = cos_sim[labels == 1].mean().item() if (labels == 1).any() else None\n",
    "                    if normal_sim is not None: epoch_stats[\"normal_sim\"].append(normal_sim)\n",
    "                    if anomaly_sim is not None: epoch_stats[\"anomaly_sim\"].append(anomaly_sim)\n",
    "\n",
    "                    anomaly_scores = 1 - cos_sim\n",
    "                    probs = anomaly_scores\n",
    "                    preds = (anomaly_scores > current_threshold).long()\n",
    "                    loss = criterion(embeddings, normal_proto, labels)\n",
    "\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported head_mode: {head_mode}\")\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * stft.size(0)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.detach().cpu().numpy())\n",
    "            all_preds.extend(preds.detach().cpu().numpy())\n",
    "\n",
    "        print(f\"[DEBUG] Train label counts (epoch {epoch+1}): {class_couns_train}\")\n",
    "        if epoch_stats['normal_sim']:\n",
    "            avg_normal_sim = sum(epoch_stats[\"normal_sim\"]) / len(epoch_stats[\"normal_sim\"])\n",
    "            avg_anomaly_sim = sum(epoch_stats[\"anomaly_sim\"]) / len(epoch_stats[\"anomaly_sim\"])\n",
    "            print(f\"[DEBUG] Average Normal CosSim: {avg_normal_sim:.4f} | Average Anomaly CosSim: {avg_anomaly_sim:.4f}\")\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_auc = roc_auc_score(all_labels, all_probs) if len(np.unique(all_labels)) > 1 else float('nan')\n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "        train_bacc = balanced_accuracy_score(all_labels, all_preds)\n",
    "        train_aucs.append(train_auc)\n",
    "        train_accs.append(train_acc)\n",
    "        train_baccs.append(train_bacc)\n",
    "\n",
    "        print(f\"Train Loss: {epoch_loss:.4f} | Train AUC: {train_auc:.4f} | Train Acc: {train_acc:.4f} | Train BAcc: {train_bacc:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_auc, val_acc, val_bacc, _, _, _, current_optimal_threshold = evaluate_model(\n",
    "            model, val_loader, criterion, phase=\"Validation\", device=device,head_mode=head_mode, sample_count=5, threshold=current_threshold, use_temporal=use_temporal, aux_seq_weight=aux_seq_weight\n",
    "        )\n",
    "        val_losses.append(val_loss)\n",
    "        val_aucs.append(val_auc)\n",
    "        val_accs.append(val_acc)\n",
    "        val_baccs.append(val_bacc)\n",
    "\n",
    "        if schedular is not None:\n",
    "            try:\n",
    "                schedular.step()\n",
    "            except Exception as error:\n",
    "                pass\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Learning Rate = {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        # Save by best loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            loss_path = model_save_path.replace(\".pth\", \"_best_loss.pth\")\n",
    "            torch.save(model.state_dict(), loss_path)\n",
    "            print(f\"Saved Best-Loss model to {loss_path} (val_loss improved to {best_val_loss:.4f})\")\n",
    "\n",
    "        # Save by best AUC\n",
    "        if not np.isnan(val_auc) and val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_threshold = current_optimal_threshold\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Saved Best-AUC model to {model_save_path} (val_auc improved to {best_val_auc:.4f})\")\n",
    "        else:\n",
    "            print(f\"Val AUC {val_auc:.4f} did not improve from {best_val_auc:.4f}\")\n",
    "\n",
    "    if save_plots:\n",
    "        epochs = range(1, num_epochs+1)\n",
    "        plt.figure(figsize=(18,4))\n",
    "        plt.subplot(1,4,1); plt.plot(epochs, train_losses, label='Train Loss'); plt.plot(epochs, val_losses, label='Val Loss'); plt.legend(); plt.grid(True); plt.title(\"Train / Validation Loss\")\n",
    "        plt.subplot(1,4,2); plt.plot(epochs, train_aucs, label='Train AUC'); plt.plot(epochs, val_aucs, label='Val AUC'); plt.legend(); plt.grid(True); plt.title(\"Train / Validation AUC\")\n",
    "        plt.subplot(1,4,3); plt.plot(epochs, train_accs, label='Train ACC'); plt.plot(epochs, val_accs, label='Val ACC'); plt.legend(); plt.grid(True); plt.title(\"Train / Validation Accuracy\")\n",
    "        plt.subplot(1,4,4); plt.plot(epochs, train_baccs, label='Train BACC'); plt.plot(epochs, val_baccs, label='Val BACC'); plt.legend(); plt.grid(True); plt.title(\"Train / Validation BACC\")\n",
    "        plt.tight_layout(); plt.savefig(os.path.join(save_path, \"Training_summpary.png\")); plt.show(); plt.close()\n",
    "\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9c3b89",
   "metadata": {},
   "source": [
    "# Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ece14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Main\n",
    "# =========================================================\n",
    "def main():\n",
    "    SEED = 42\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Applying the Transformations\n",
    "    train_transform = ComposeT([\n",
    "        ToTensor(),\n",
    "        SpecTimePitchWarp(max_time_scale=1.1, max_freq_scale=1.1),\n",
    "        SpecAugment(freq_mask_param=2, time_mask_param=2, n_freq_masks=1, n_time_masks=1),\n",
    "    ])\n",
    "\n",
    "    no_transform = ComposeT([\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # Base Paired dataset\n",
    "    base_dataset = PairedSpectrogramDataset(base_dir=FEATURES_DIR, transform=None)\n",
    "    all_labels = [int(x) for x in base_dataset.labels]\n",
    "\n",
    "    # Wrapping with windowed dataset if temporal\n",
    "    if USE_TEMPORAL_DECODER:\n",
    "        wrapped_dataset = WindowedPairedSpectrogramDataset(base_dataset=base_dataset, window_size=WINDOW_SIZE)\n",
    "        effective_len = len(wrapped_dataset)\n",
    "\n",
    "        windowed_labels = [all_labels[i] for i in range(effective_len)]\n",
    "\n",
    "        idxs = list(range(effective_len))\n",
    "        train_idx, temp_idx = train_test_split(idxs, test_size=0.3, stratify=windowed_labels, random_state=42)\n",
    "        val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, stratify=[windowed_labels[i] for i in temp_idx], random_state=42)\n",
    "\n",
    "        train_base = PairedSpectrogramDataset(base_dir=FEATURES_DIR, transform=train_transform)\n",
    "        val_base = PairedSpectrogramDataset(base_dir=FEATURES_DIR, transform=no_transform)\n",
    "        test_base = PairedSpectrogramDataset(base_dir=FEATURES_DIR, transform=None)\n",
    "\n",
    "        train_set = Subset(WindowedPairedSpectrogramDataset(base_dataset=train_base, window_size=WINDOW_SIZE), train_idx)\n",
    "        val_set = Subset(WindowedPairedSpectrogramDataset(base_dataset=val_base, window_size=WINDOW_SIZE), val_idx)\n",
    "        test_set = Subset(WindowedPairedSpectrogramDataset(base_dataset=test_base, window_size=WINDOW_SIZE), test_idx)\n",
    "    \n",
    "    else:\n",
    "        # Stratified split indices of base dataset\n",
    "        idxs = list(range(len(base_dataset)))\n",
    "        train_idx, temp_idx = train_test_split(idxs, test_size=0.3, stratify=all_labels, random_state=42)\n",
    "        val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, stratify=[all_labels[i] for i in temp_idx], random_state=42)\n",
    "\n",
    "        train_base = PairedSpectrogramDataset(base_dir=FEATURES_DIR, transform=train_transform)\n",
    "        val_base = PairedSpectrogramDataset(base_dir=FEATURES_DIR, transform=no_transform)\n",
    "        test_base = PairedSpectrogramDataset(base_dir=FEATURES_DIR, transform=no_transform)\n",
    "\n",
    "        train_set = Subset(train_base, train_idx)\n",
    "        val_set = Subset(val_base, val_idx)\n",
    "        test_set = Subset(test_base, test_idx)\n",
    "\n",
    "    # Data Loaders\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, drop_last=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    print(f\"Splir Sizes => Train: {len(train_set)}, Val: {len(val_set)}, Test: {len(test_set)}\")\n",
    "    print(f\"Label Distribution (Train): {Counter([int(base_dataset[i]['label']) for i in train_idx])}\")\n",
    "    print(f\"Lable Distribution (Valdation): {Counter([int(base_dataset[i]['label']) for i in val_idx])}\")\n",
    "    print(f\"Label Distribution (Test): {Counter([int(base_dataset[i]['label']) for i in test_idx])}\")\n",
    "\n",
    "    head_mode = HEAD_MODE.lower()\n",
    "\n",
    "    # Configure head + criterion\n",
    "    if head_mode == \"prototype\":\n",
    "        # Note: AnomalyScorer('prototype') returns a distance score.\n",
    "        # Use BCE/ BinaryFocalLoss on this score (Not ContrastiveLoss)\n",
    "        head = AnomalyScorer(in_dim=256, dropout=0.4, mode='prototype')\n",
    "        pos_count = sum(all_labels); neg_count = len(all_labels) - pos_count\n",
    "        pos_weight = torch.tensor([neg_count / (pos_count + 1e-8)], dtype=torch.float32).to(device)\n",
    "        criterion = BinaryFocalLoss(alpha=0.25, gamma=2.0, pos_weight=pos_weight, reduction='mean')\n",
    "        # print(f\"Using head: {head}\")\n",
    "        print(\"\\nTransformations Applied to Train Set\")\n",
    "        for transform in train_transform.transforms:\n",
    "            # Print the name of the transformation class\n",
    "            print(f\"  - {transform.__class__.__name__}\")\n",
    "    \n",
    "            # Check for specific transformations and print their parameters\n",
    "            if isinstance(transform, SpecTimePitchWarp):\n",
    "\n",
    "                print(f\"    - time_scale: {getattr(transform, 'max_time_scale', {transform.max_time})}\")\n",
    "                print(f\"    - freq_scale: {getattr(transform, 'max_freq_scale', {transform.max_freq})}\")\n",
    "            if isinstance(transform, SpecAugment):\n",
    "                print(f\"    - freq_mask_param: {getattr(transform,'freq_mask_param',{transform.fm})}\")\n",
    "                print(f\"    - time_mask_param: {getattr(transform,'time_mask_param',{transform.tm})}\")\n",
    "                print(f\"    - n_freq_masks: {getattr(transform,'n_freq_masks', {transform.nf})}\")\n",
    "                print(f\"    - n_time_masks: {getattr(transform,'n_time_masks', {transform.nt})}\")\n",
    "        print(\"Transformation's Applied to Validation/Test Set\")\n",
    "        for transform in no_transform.transforms:\n",
    "            # Print the name of the transformation class\n",
    "            print(f\"  - {transform.__class__.__name__}\")\n",
    "    \n",
    "            # Check for specific transformations and print their parameters\n",
    "            if isinstance(transform, SpecTimePitchWarp):\n",
    "\n",
    "                print(f\"    - time_scale: {getattr(transform, 'max_time_scale', {transform.max_time})}\")\n",
    "                print(f\"    - freq_scale: {getattr(transform, 'max_freq_scale', {transform.max_freq})}\")\n",
    "            if isinstance(transform, SpecAugment):\n",
    "                print(f\"    - freq_mask_param: {getattr(transform,'freq_mask_param',{transform.fm})}\")\n",
    "                print(f\"    - time_mask_param: {getattr(transform,'time_mask_param',{transform.tm})}\")\n",
    "                print(f\"    - n_freq_masks: {getattr(transform,'n_freq_masks', {transform.nf})}\")\n",
    "                print(f\"    - n_time_masks: {getattr(transform,'n_time_masks', {transform.nt})}\")\n",
    "    \n",
    "    elif head_mode == \"mlp\":\n",
    "        head = ComplexAnomalyMLP(in_dim=256, dropout=0.4, out_dim=1)\n",
    "        pos_count = sum(all_labels); neg_count = len(all_labels) - pos_count\n",
    "        pos_weight = torch.tensor([neg_count / (pos_count + 1e-8)], dtype=torch.float32).to(device)\n",
    "        criterion = BinaryFocalLoss(alpha=0.25, gamma=2.0, pos_weight=pos_weight, reduction='mean')\n",
    "        # print(f\"Used Head: {head}\")\n",
    "        print(\"\\nTransformations Applied to Train Set\")\n",
    "        for transform in train_transform.transforms:\n",
    "            # Print the name of the transformation class\n",
    "            print(f\"  - {transform.__class__.__name__}\")\n",
    "    \n",
    "            # Check for specific transformations and print their parameters\n",
    "            if isinstance(transform, SpecTimePitchWarp):\n",
    "\n",
    "                print(f\"    - time_scale: {getattr(transform, 'max_time_scale', {transform.max_time})}\")\n",
    "                print(f\"    - freq_scale: {getattr(transform, 'max_freq_scale', {transform.max_freq})}\")\n",
    "            if isinstance(transform, SpecAugment):\n",
    "                print(f\"    - freq_mask_param: {getattr(transform,'freq_mask_param',{transform.fm})}\")\n",
    "                print(f\"    - time_mask_param: {getattr(transform,'time_mask_param',{transform.tm})}\")\n",
    "                print(f\"    - n_freq_masks: {getattr(transform,'n_freq_masks', {transform.nf})}\")\n",
    "                print(f\"    - n_time_masks: {getattr(transform,'n_time_masks', {transform.nt})}\")\n",
    "        print(\"Transformation's Applied to Validation/Test Set\")\n",
    "        for transform in no_transform.transforms:\n",
    "            # Print the name of the transformation class\n",
    "            print(f\"  - {transform.__class__.__name__}\")\n",
    "    \n",
    "            # Check for specific transformations and print their parameters\n",
    "            if isinstance(transform, SpecTimePitchWarp):\n",
    "\n",
    "                print(f\"    - time_scale: {getattr(transform, 'max_time_scale', {transform.max_time})}\")\n",
    "                print(f\"    - freq_scale: {getattr(transform, 'max_freq_scale', {transform.max_freq})}\")\n",
    "            if isinstance(transform, SpecAugment):\n",
    "                print(f\"    - freq_mask_param: {getattr(transform,'freq_mask_param',{transform.fm})}\")\n",
    "                print(f\"    - time_mask_param: {getattr(transform,'time_mask_param',{transform.tm})}\")\n",
    "                print(f\"    - n_freq_masks: {getattr(transform,'n_freq_masks', {transform.nf})}\")\n",
    "                print(f\"    - n_time_masks: {getattr(transform,'n_time_masks', {transform.nt})}\")\n",
    "    \n",
    "    elif head_mode == \"embedding\":\n",
    "        head = EmbeddingMLP(in_dim=256, hidden=128, dropout=0.4, emb_dim=EMB_DIM)\n",
    "        criterion = ContrastiveLoss(margin=CONTRASTIVE_MARGIN)\n",
    "        # print(f\"Used head: {head}\")\n",
    "        print(\"\\nTransformations Applied to Train Set\")\n",
    "        for transform in train_transform.transforms:\n",
    "            # Print the name of the transformation class\n",
    "            print(f\"  - {transform.__class__.__name__}\")\n",
    "    \n",
    "            # Check for specific transformations and print their parameters\n",
    "            if isinstance(transform, SpecTimePitchWarp):\n",
    "\n",
    "                print(f\"    - time_scale: {getattr(transform, 'max_time_scale', {transform.max_time})}\")\n",
    "                print(f\"    - freq_scale: {getattr(transform, 'max_freq_scale', {transform.max_freq})}\")\n",
    "            if isinstance(transform, SpecAugment):\n",
    "                print(f\"    - freq_mask_param: {getattr(transform,'freq_mask_param',{transform.fm})}\")\n",
    "                print(f\"    - time_mask_param: {getattr(transform,'time_mask_param',{transform.tm})}\")\n",
    "                print(f\"    - n_freq_masks: {getattr(transform,'n_freq_masks', {transform.nf})}\")\n",
    "                print(f\"    - n_time_masks: {getattr(transform,'n_time_masks', {transform.nt})}\")\n",
    "        print(\"Transformation's Applied to Validation/Test Set\")\n",
    "        for transform in no_transform.transforms:\n",
    "            # Print the name of the transformation class\n",
    "            print(f\"  - {transform.__class__.__name__}\")\n",
    "    \n",
    "            # Check for specific transformations and print their parameters\n",
    "            if isinstance(transform, SpecTimePitchWarp):\n",
    "\n",
    "                print(f\"    - time_scale: {getattr(transform, 'max_time_scale', {transform.max_time})}\")\n",
    "                print(f\"    - freq_scale: {getattr(transform, 'max_freq_scale', {transform.max_freq})}\")\n",
    "            if isinstance(transform, SpecAugment):\n",
    "                print(f\"    - freq_mask_param: {getattr(transform,'freq_mask_param',{transform.fm})}\")\n",
    "                print(f\"    - time_mask_param: {getattr(transform,'time_mask_param',{transform.tm})}\")\n",
    "                print(f\"    - n_freq_masks: {getattr(transform,'n_freq_masks', {transform.nf})}\")\n",
    "                print(f\"    - n_time_masks: {getattr(transform,'n_time_masks', {transform.nt})}\")\n",
    "    \n",
    "    elif head_mode == \"classifier\":\n",
    "        head = SimpleAnomalyMLP(in_dim=256, dropout=0.4, hidden=128, out_dim=2)\n",
    "        class_counts = [2624, 319]\n",
    "        alpha = 0.7\n",
    "        total = sum(class_counts)\n",
    "        class_weights = [(total /c ) ** alpha for c in class_counts]\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        # print(f\"Used head: {head}\")\n",
    "        print(\"\\nTransformations Applied to Train Set\")\n",
    "        for transform in train_transform.transforms:\n",
    "            # Print the name of the transformation class\n",
    "            print(f\"  - {transform.__class__.__name__}\")\n",
    "    \n",
    "            # Check for specific transformations and print their parameters\n",
    "            if isinstance(transform, SpecTimePitchWarp):\n",
    "\n",
    "                print(f\"    - time_scale: {getattr(transform, 'max_time_scale', {transform.max_time})}\")\n",
    "                print(f\"    - freq_scale: {getattr(transform, 'max_freq_scale', {transform.max_freq})}\")\n",
    "            if isinstance(transform, SpecAugment):\n",
    "                print(f\"    - freq_mask_param: {getattr(transform,'freq_mask_param',{transform.fm})}\")\n",
    "                print(f\"    - time_mask_param: {getattr(transform,'time_mask_param',{transform.tm})}\")\n",
    "                print(f\"    - n_freq_masks: {getattr(transform,'n_freq_masks', {transform.nf})}\")\n",
    "                print(f\"    - n_time_masks: {getattr(transform,'n_time_masks', {transform.nt})}\")\n",
    "        print(\"Transformation's Applied to Validation/Test Set\")\n",
    "        for transform in no_transform.transforms:\n",
    "            # Print the name of the transformation class\n",
    "            print(f\"  - {transform.__class__.__name__}\")\n",
    "    \n",
    "            # Check for specific transformations and print their parameters\n",
    "            if isinstance(transform, SpecTimePitchWarp):\n",
    "\n",
    "                print(f\"    - time_scale: {getattr(transform, 'max_time_scale', {transform.max_time})}\")\n",
    "                print(f\"    - freq_scale: {getattr(transform, 'max_freq_scale', {transform.max_freq})}\")\n",
    "            if isinstance(transform, SpecAugment):\n",
    "                print(f\"    - freq_mask_param: {getattr(transform,'freq_mask_param',{transform.fm})}\")\n",
    "                print(f\"    - time_mask_param: {getattr(transform,'time_mask_param',{transform.tm})}\")\n",
    "                print(f\"    - n_freq_masks: {getattr(transform,'n_freq_masks', {transform.nf})}\")\n",
    "                print(f\"    - n_time_masks: {getattr(transform,'n_time_masks', {transform.nt})}\")\n",
    "    \n",
    "    elif head_mode == \"classifier-1\":\n",
    "        head = AnomalyScorer(in_dim=256, dropout=0.4, mode='classifier-1')\n",
    "        pos_count = sum(all_labels); neg_count = len(all_labels) - pos_count\n",
    "        pos_weight = torch.tensor([neg_count / (pos_count + 1e-8)], dtype=torch.float32).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        # print(f\"Using head: {head}\")\n",
    "        print(\"\\nTransformations Applied to Train Set\")\n",
    "        for transform in train_transform.transforms:\n",
    "            # Print the name of the transformation class\n",
    "            print(f\"  - {transform.__class__.__name__}\")\n",
    "    \n",
    "            # Check for specific transformations and print their parameters\n",
    "            if isinstance(transform, SpecTimePitchWarp):\n",
    "\n",
    "                print(f\"    - time_scale: {getattr(transform, 'max_time_scale', {transform.max_time})}\")\n",
    "                print(f\"    - freq_scale: {getattr(transform, 'max_freq_scale', {transform.max_freq})}\")\n",
    "            if isinstance(transform, SpecAugment):\n",
    "                print(f\"    - freq_mask_param: {getattr(transform,'freq_mask_param',{transform.fm})}\")\n",
    "                print(f\"    - time_mask_param: {getattr(transform,'time_mask_param',{transform.tm})}\")\n",
    "                print(f\"    - n_freq_masks: {getattr(transform,'n_freq_masks', {transform.nf})}\")\n",
    "                print(f\"    - n_time_masks: {getattr(transform,'n_time_masks', {transform.nt})}\")\n",
    "        print(\"Transformation's Applied to Validation/Test Set\")\n",
    "        for transform in no_transform.transforms:\n",
    "            # Print the name of the transformation class\n",
    "            print(f\"  - {transform.__class__.__name__}\")\n",
    "    \n",
    "            # Check for specific transformations and print their parameters\n",
    "            if isinstance(transform, SpecTimePitchWarp):\n",
    "\n",
    "                print(f\"    - time_scale: {getattr(transform, 'max_time_scale', {transform.max_time})}\")\n",
    "                print(f\"    - freq_scale: {getattr(transform, 'max_freq_scale', {transform.max_freq})}\")\n",
    "            if isinstance(transform, SpecAugment):\n",
    "                print(f\"    - freq_mask_param: {getattr(transform,'freq_mask_param',{transform.fm})}\")\n",
    "                print(f\"    - time_mask_param: {getattr(transform,'time_mask_param',{transform.tm})}\")\n",
    "                print(f\"    - n_freq_masks: {getattr(transform,'n_freq_masks', {transform.nf})}\")\n",
    "                print(f\"    - n_time_masks: {getattr(transform,'n_time_masks', {transform.nt})}\")\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid Head_MODE\")\n",
    "    \n",
    "    # Build Model \n",
    "    model = FusedModel(\n",
    "        stft_dim=512, cqt_dim=320, fusion_dim=256,\n",
    "        head= head, head_mode=head_mode, use_decoder=USE_TEMPORAL_DECODER, temporal_hidden=64\n",
    "    ).to(device)\n",
    "    # print(f\"\\nUsing the Model: {model}\\n\")\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=NUM_EPOCHs, eta_min=1e-6)\n",
    "\n",
    "    model_path = os.path.join(save_path, \"best_model.pth\")\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "\n",
    "    best_threshold = train_model(model=model, train_loader=train_loader, val_loader=val_loader, criterion=criterion, optimizer=optimizer,\n",
    "                                 head_mode=head_mode, schedular=scheduler, num_epochs=NUM_EPOCHs, model_save_path=model_path,\n",
    "                                 device=device, save_plots=True, use_temporal=USE_TEMPORAL_DECODER, aux_seq_weight=SEQ_LOSS_WEIGHT\n",
    "                                )\n",
    "    \n",
    "    print(\"\\n--- Final Test Evaluation ---\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "    safe_threshold = float(best_threshold) if best_threshold is not None else 0.5\n",
    "\n",
    "    test_loss, test_auc, test_acc, test_bacc, test_f1, all_labels_test, all_probs_test, _ = evaluate_model(\n",
    "        model=model, data_loader=test_loader, criterion=criterion, phase=\"Test\", device=device, head_mode=head_mode,sample_count=5, threshold=safe_threshold, \n",
    "        use_temporal=USE_TEMPORAL_DECODER, aux_seq_weight=SEQ_LOSS_WEIGHT\n",
    "    )\n",
    "    print(f\"\\nFinal Test Metrics (With best Validation threshold): {best_threshold:.2f}\")\n",
    "    print(f\"Loss: {test_loss:.4f} | AUC: {test_auc:.4f} | Accuracy: {test_acc:.4f} | Balanced Accuracy: {test_bacc:.4f} | F1-Score: {test_f1:.4f}\")\n",
    "\n",
    "    if len(np.unique(all_labels_test)) > 1:\n",
    "        final_pauc = calculate_pAUC(labels=all_labels_test, preds = all_probs_test, max_fpr=0.2)\n",
    "    else:\n",
    "        print(\"Test set contains only one class; cannot compute pAUC\")\n",
    "    \n",
    "    # ROC\n",
    "    fpr, tpr, _ = roc_curve(y_true=all_labels_test, y_score=all_probs_test)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"{test_auc:.4f}\")\n",
    "    plt.plot([0,1],[0,1], linestyle='--', lw=1)\n",
    "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"Test ROC With Optimal Threshold\"); plt.legend()\n",
    "    plt.grid(True); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, \"roc_test_optimal.png\")); plt.close()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    labels = [\"Normal\", \"Abnormal\"]\n",
    "    all_preds_test = (np.array(all_probs_test) > safe_threshold).astype(int)\n",
    "    plot_confusion_matrix(y_true=all_labels_test, y_pred=all_preds_test, labels=labels, save_path=save_path, title=\"Test Set Confusion Matrix\")\n",
    "\n",
    "    # GradCAM\n",
    "    try:\n",
    "        cams = build_gradcam_for_model(model=model, device=device)\n",
    "        cam_out_dir = os.path.join(save_path, 'gradcam')\n",
    "        ref_set = test_set\n",
    "        run_and_save_gradcams(model=model, cams=cams, dataset=ref_set, device=device, out_dir=cam_out_dir, n_samples=5)\n",
    "    except Exception as error:\n",
    "        print(f\"GradCAM step Failed: {error}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
